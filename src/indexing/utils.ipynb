{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71487809",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Dict, List\n",
    "from dotenv import load_dotenv\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1279fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa30e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# Utility Functions\n",
    "# ---------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03746fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_exists(index_name: str) -> bool:\n",
    "    return index_name in pc.list_indexes().names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb53aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def namespace_exists(index_name: str, namespace: str) -> bool:\n",
    "    idx = pc.Index(index_name)\n",
    "    stats = idx.describe_index_stats()\n",
    "    return namespace in stats.get(\"namespaces\", {})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfa5a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_documents(path: str) -> List[str]:\n",
    "    files = []\n",
    "    for root, _, filenames in os.walk(path):\n",
    "        for f in filenames:\n",
    "            if f.lower().endswith((\".txt\", \".md\", \".pdf\")):\n",
    "                files.append(os.path.join(root, f))\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d380b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# Chunking & Embedding\n",
    "# ---------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_chunking(path: str, chunk_size: int, chunk_overlap: int) -> List[str]:\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "\n",
    "    chunks = []\n",
    "    for file in list_documents(path):\n",
    "        with open(file, \"r\", errors=\"ignore\") as f:\n",
    "            text = f.read()\n",
    "            chunks.extend(splitter.split_text(text))\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_chunks(chunks: List[str], embedding_model: str):\n",
    "    embeddings = OpenAIEmbeddings(model=embedding_model)\n",
    "    return embeddings.embed_documents(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# Index Creation\n",
    "# ---------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dafc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index_if_needed(config: Dict):\n",
    "    index_name = config[\"index_name\"]\n",
    "\n",
    "    if index_exists(index_name):\n",
    "        return  # already exists\n",
    "\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=config[\"dimensions\"],\n",
    "        metric=config[\"metric\"],\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_namespace_if_needed(config: Dict):\n",
    "    index_name = config[\"index_name\"]\n",
    "    namespace = config[\"namespace_name\"]\n",
    "\n",
    "    if not namespace_exists(index_name, namespace):\n",
    "        # Initialize namespace (Pinecone auto-creates on first upsert)\n",
    "        idx = pc.Index(index_name)\n",
    "        idx.upsert(\n",
    "            vectors=[(\"init-vector\", [0.0] * config[\"dimensions\"])],\n",
    "            namespace=namespace\n",
    "        )\n",
    "        idx.delete(ids=[\"init-vector\"], namespace=namespace)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311dc2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# Indexing Pipeline\n",
    "# ---------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431175b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_documents(config: Dict):\n",
    "    index_name = config[\"index_name\"]\n",
    "    namespace = config[\"namespace_name\"]\n",
    "\n",
    "    reindex = not config[\"skip_if_namespace_exists\"]\n",
    "\n",
    "    # Case 1 — namespace exists & skip = True\n",
    "    if namespace_exists(index_name, namespace) and config[\"skip_if_namespace_exists\"]:\n",
    "        return\n",
    "\n",
    "    # Case 2 — namespace exists & skip = False → reindex\n",
    "    if namespace_exists(index_name, namespace) and reindex:\n",
    "        delete_namespace(config)\n",
    "\n",
    "    # Guarantee namespace exists before indexing\n",
    "    create_namespace_if_needed(config)\n",
    "\n",
    "    # Get content\n",
    "    chunks = recursive_chunking(\n",
    "        config[\"documents_path\"],\n",
    "        config[\"chunk_size\"],\n",
    "        config[\"chunk_overlap\"]\n",
    "    )\n",
    "\n",
    "    vectors = embed_chunks(chunks, config[\"embedding_model\"])\n",
    "\n",
    "    # Format for Pinecone\n",
    "    payload = [(f\"id-{i}\", vectors[i], {\"text\": chunks[i]}) for i in range(len(chunks))]\n",
    "\n",
    "    # Upsert\n",
    "    idx = pc.Index(index_name)\n",
    "    idx.upsert(vectors=payload, namespace=namespace)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6e2d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# Delete Functions\n",
    "# ---------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cbee8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_namespace(config: Dict):\n",
    "    idx = pc.Index(config[\"index_name\"])\n",
    "    idx.delete(delete_all=True, namespace=config[\"namespace_name\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41ce91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_index(config: Dict):\n",
    "    index_name = config[\"index_name\"]\n",
    "    if index_exists(index_name):\n",
    "        pc.delete_index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434c3575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# Update (Reindex)\n",
    "# ---------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8a7428",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Pinecone Index Manager\n",
    "Production-ready, no logging, config-driven\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from typing import Dict, List\n",
    "from dotenv import load_dotenv\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Utility Functions\n",
    "# ---------------------------------------------\n",
    "\n",
    "def index_exists(index_name: str) -> bool:\n",
    "    return index_name in pc.list_indexes().names()\n",
    "\n",
    "\n",
    "def namespace_exists(index_name: str, namespace: str) -> bool:\n",
    "    idx = pc.Index(index_name)\n",
    "    stats = idx.describe_index_stats()\n",
    "    return namespace in stats.get(\"namespaces\", {})\n",
    "\n",
    "\n",
    "def list_documents(path: str) -> List[str]:\n",
    "    files = []\n",
    "    for root, _, filenames in os.walk(path):\n",
    "        for f in filenames:\n",
    "            if f.lower().endswith((\".txt\", \".md\", \".pdf\")):\n",
    "                files.append(os.path.join(root, f))\n",
    "    return files\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Chunking & Embedding\n",
    "# ---------------------------------------------\n",
    "\n",
    "def get_chunks(path: str, chunk_size: int, chunk_overlap: int) -> List[str]:\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "\n",
    "    chunks = []\n",
    "    for file in list_documents(path):\n",
    "        with open(file, \"r\", errors=\"ignore\") as f:\n",
    "            text = f.read()\n",
    "            chunks.extend(splitter.split_text(text))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def embed_chunks(chunks: List[str], embedding_model: str):\n",
    "    embeddings = OpenAIEmbeddings(model=embedding_model)\n",
    "    return embeddings.embed_documents(chunks)\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Index Creation\n",
    "# ---------------------------------------------\n",
    "\n",
    "def create_index_if_needed(config: Dict):\n",
    "    index_name = config[\"index_name\"]\n",
    "\n",
    "    if index_exists(index_name):\n",
    "        return  # already exists\n",
    "\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=config[\"dimensions\"],\n",
    "        metric=config[\"metric\"],\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
    "    )\n",
    "\n",
    "\n",
    "def create_namespace_if_needed(config: Dict):\n",
    "    index_name = config[\"index_name\"]\n",
    "    namespace = config[\"namespace_name\"]\n",
    "\n",
    "    if not namespace_exists(index_name, namespace):\n",
    "        # Initialize namespace (Pinecone auto-creates on first upsert)\n",
    "        idx = pc.Index(index_name)\n",
    "        idx.upsert(\n",
    "            vectors=[(\"init-vector\", [0.0] * config[\"dimensions\"])],\n",
    "            namespace=namespace\n",
    "        )\n",
    "        idx.delete(ids=[\"init-vector\"], namespace=namespace)\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Indexing Pipeline\n",
    "# ---------------------------------------------\n",
    "\n",
    "def index_documents(config: Dict):\n",
    "    index_name = config[\"index_name\"]\n",
    "    namespace = config[\"namespace_name\"]\n",
    "\n",
    "    reindex = not config[\"skip_if_namespace_exists\"]\n",
    "\n",
    "    # Case 1 — namespace exists & skip = True\n",
    "    if namespace_exists(index_name, namespace) and config[\"skip_if_namespace_exists\"]:\n",
    "        return\n",
    "\n",
    "    # Case 2 — namespace exists & skip = False → reindex\n",
    "    if namespace_exists(index_name, namespace) and reindex:\n",
    "        delete_namespace(config)\n",
    "\n",
    "    # Guarantee namespace exists before indexing\n",
    "    create_namespace_if_needed(config)\n",
    "\n",
    "    # Get content\n",
    "    chunks = get_chunks(\n",
    "        config[\"documents_path\"],\n",
    "        config[\"chunk_size\"],\n",
    "        config[\"chunk_overlap\"]\n",
    "    )\n",
    "\n",
    "    vectors = embed_chunks(chunks, config[\"embedding_model\"])\n",
    "\n",
    "    # Format for Pinecone\n",
    "    payload = [(f\"id-{i}\", vectors[i], {\"text\": chunks[i]}) for i in range(len(chunks))]\n",
    "\n",
    "    # Upsert\n",
    "    idx = pc.Index(index_name)\n",
    "    idx.upsert(vectors=payload, namespace=namespace)\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Delete Functions\n",
    "# ---------------------------------------------\n",
    "\n",
    "def delete_namespace(config: Dict):\n",
    "    idx = pc.Index(config[\"index_name\"])\n",
    "    idx.delete(delete_all=True, namespace=config[\"namespace_name\"])\n",
    "\n",
    "\n",
    "def delete_index(config: Dict):\n",
    "    index_name = config[\"index_name\"]\n",
    "    if index_exists(index_name):\n",
    "        pc.delete_index(index_name)\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Update (Reindex)\n",
    "# ---------------------------------------------\n",
    "\n",
    "def update_index(config: Dict, new_config: Dict):\n",
    "    \"\"\"\n",
    "    Update index or namespace content. \n",
    "    new_config contains new doc path, chunking, embedding model, etc.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure index exists\n",
    "    create_index_if_needed(new_config)\n",
    "\n",
    "    # Reindex only the namespace\n",
    "    delete_namespace(new_config)\n",
    "    index_documents(new_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
